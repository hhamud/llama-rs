
pub fn interactive_mode(model: &llama_rs::Model, vocab: &llama_rs::Vocabulary){
    println!("activated")
    // create a sliding window of context
    // convert initial prompt into tokens
    // convert ai answer into tokens and add into total token count
    // wait for user response
    // repeat
    // issue a warning after the total context is > 2048 tokens

}
